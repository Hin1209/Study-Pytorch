{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKbtM8TWGu3vGDSeu8uc9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tin1209/Study-Pytorch/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9huB5-xiQx8W"
      },
      "source": [
        "# 선형회귀분석(Linear Regression)   \n",
        "   \n",
        "   \n",
        "## 1. 선형회귀분석이란?   \n",
        "\n",
        "선형회귀분석이란 주어진 데이터를 가장 잘 설명하는 하나의 직선을 찾는 것이다.  \n",
        "하나의 독립변수에 대하여 선형회귀분석을 하는 경우 __단순선형회귀(simple linear regression)__라 하며,   \n",
        "여러개의 독립변수에 대하여 선형회귀분석을 하는 경우 __다중선형회귀(multivariate linear regression)__이라 한다.  \n",
        "\n",
        "단순선형회귀분석은 $x$와 $y$라는 데이터가 주어졌을때, 데이터를 가장 잘 표현하는 $y = wx + b$ 꼴의 직선을 찾는 것이다.   \n",
        "여기서 $w$는 __가중치(weight)__, $b$는 __편차(bias)__라 한다.    \n",
        "\n",
        "\n",
        "## 2. 손실 함수 및 경사하강법   \n",
        "  \n",
        "\n",
        "데이터를 가장 잘 표현한다는 말을 수학적으로 표현하면 $y = wx + b$ 를 통해 예측한 $\\hat{y}$ 와 실제 결과값 $y$를 비교하는 것이다.   \n",
        "이때 비교하는 방법 중 대표적인 것으로 __평균제곱오차(mean squared error-MSE)__가 있다.  \n",
        "평균제곱오차 식은 다음과 같다.  $$MSE = \\frac{1} {n}  \\sum_{i = 1}^n (\\hat{y} - y)^2$$   \n",
        "이렇게 오차를 나타내는 함수를 흔히 __손실 함수(Loss function)__ 또는 __비용 함수(Cost function)__ 이라고 한다. MSE는 이러한 Cost function 중에 하나이다. \n",
        "\n",
        "여기서 Cost function을 최소화하는 $w$와 $b$를 찾기위해 __경사하강법(Gradient descent)__이라는 방법을 사용한다. 여기서 경사란 함수의 기울기를 의미한다.   \n",
        "Cost function에서 주어진 $w$에 대한 기울기를 구하고, 그 기울기를 바탕으로 다음 $w$를 업데이트 함으로써 오차의 극솟값을 찾아준다. 식으로 표현하면 다음과 같다.\n",
        "$$ w_{t+1} = w_{t} - gradient * learning\\ rate $$  \n",
        "여기서 __학습률(learning rate)__이 새로 나오는데, 학습률은 변수 $w$를 얼마나 업데이트할지 결정하는 수치이다.   \n",
        "\n",
        "## 3. 파이토치에서 경사하강법  \n",
        "\n",
        "파이토치에서는 데이터의 기본 단위로 다차원 배열인 텐서(Tensor)를 사용한다.   \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QRjUegDSrMy",
        "outputId": "1841c562-cea9-4580-c0cd-ed8ebddb9d82"
      },
      "source": [
        "import torch\n",
        "X = torch.Tensor(2, 3)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.8181e-35, 0.0000e+00, 3.9236e-44],\n",
            "        [0.0000e+00,        nan, 0.0000e+00]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npn1mpufdZr5"
      },
      "source": [
        "위의 코드를 실행시키면 1행에서는 파이토치의 프레임워크를 불러오고,  \n",
        "2행에서는 X라는 변수에 임의의 난수를 원소로 갖는 2x3 형태의 텐서를 생성해서 지정한다.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd7Z19EJdv3h",
        "outputId": "9ea4ecc2-e25e-4163-bf51-f39b3c7496a1"
      },
      "source": [
        "X = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOKDdZ9md_VW"
      },
      "source": [
        "위의 코드처럼 텐서를 생성할때 직접 shape과 element들을 지정해줄 수 있다.   \n",
        "torch.tensor 함수는 인수로 data, dtype, device, requires_grad 등을 받는다.  \n",
        "data에는 위의 코드처럼 배열이 들어가고,  \n",
        "dtype에는 데이터의 자료형이 들어간다. 자료형을 지정해주지 않을시 기본 자료형은 Float로 들어간다. \n",
        "device에는 이 텐서를 어느 기기에 올릴 것인지를 명시한다.   \n",
        "requires_grad는 이 텐서에 대한 기울기를 저장할지에 대한 여부를 지정한다.  \n",
        "\n",
        "아래의 코드는 기울기를 계산하는 코드이다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVRdFzCZe7pJ",
        "outputId": "e2d7d0a1-0518-47f7-a9ee-657888ead4ae"
      },
      "source": [
        "x = torch.tensor(data=[2.0, 3.0], requires_grad=True)\n",
        "y = x**2\n",
        "z = 2*y + 3\n",
        "\n",
        "target = torch.tensor([3.0, 4.0])\n",
        "loss = torch.sum(torch.abs(z-target))\n",
        "loss.backward()\n",
        "\n",
        "print(x.grad, y.grad, z.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 8., 12.]) None None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}