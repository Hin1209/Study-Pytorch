{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "organizational-bulgaria",
   "metadata": {},
   "source": [
    "# 1. Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "seven-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-conservative",
   "metadata": {},
   "source": [
    "# 2. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unsigned-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "num_gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-donna",
   "metadata": {},
   "source": [
    "# 3. Data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "japanese-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST(\"./data\", train=True, \n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5,), (0.5,)),\n",
    "                        ]),\n",
    "                        target_transform=None,\n",
    "                        download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-shoot",
   "metadata": {},
   "source": [
    "# 4. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polished-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                    nn.Linear(100, 7*7*256),\n",
    "                    nn.BatchNorm1d(7*7*256),\n",
    "                    nn.ReLU(),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.ConvTranspose2d(256, 128, 3, 2, 1, 1)),\n",
    "            ('bn1', nn.BatchNorm2d(128)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('conv2', nn.ConvTranspose2d(128, 64, 3, 1, 1)),\n",
    "            ('bn2', nn.BatchNorm2d(64)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        self.layer3 = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.ConvTranspose2d(64, 16, 3, 1, 1)),\n",
    "            ('bn3', nn.BatchNorm2d(16)),\n",
    "            ('relu', nn.LeakyReLU()),\n",
    "            ('conv4', nn.ConvTranspose2d(16, 1, 3, 2, 1, 1)),\n",
    "            ('relu4', nn.Tanh())\n",
    "        ]))\n",
    "  \n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = out.view(batch_size // num_gpus, 256, 7, 7)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-circuit",
   "metadata": {},
   "source": [
    "# 5. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "solid-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1, 8, 3, padding=1)),\n",
    "            #('bn1', nn.BatchNorm2d(8)),\n",
    "            ('relu1', nn.LeakyReLU()),\n",
    "            ('conv2', nn.Conv2d(8, 16, 3, padding=1)),\n",
    "            ('bn2', nn.BatchNorm2d(16)),\n",
    "            ('relu2', nn.LeakyReLU()),\n",
    "            ('max1', nn.MaxPool2d(2, 2))\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv2d(16, 32, 3, padding=1)),\n",
    "            ('bn3', nn.BatchNorm2d(32)),\n",
    "            ('relu3', nn.LeakyReLU()),\n",
    "            ('max2', nn.MaxPool2d(2, 2)),\n",
    "            ('conv4', nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            ('bn4', nn.BatchNorm2d(64)),\n",
    "            ('relu4', nn.LeakyReLU())\n",
    "        ]))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*7*7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size // num_gpus, -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-citation",
   "metadata": {},
   "source": [
    "# 6. Put instances on Multi-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "respective-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.DataParallel(Generator()).cuda()\n",
    "discriminator = nn.DataParallel(Discriminator()).cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-darkness",
   "metadata": {},
   "source": [
    "# 7. Check layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exceptional-forum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.weight\n",
      "module.layer1.0.bias\n",
      "module.layer1.1.weight\n",
      "module.layer1.1.bias\n",
      "module.layer1.1.running_mean\n",
      "module.layer1.1.running_var\n",
      "module.layer1.1.num_batches_tracked\n",
      "module.layer2.conv1.weight\n",
      "module.layer2.conv1.bias\n",
      "module.layer2.bn1.weight\n",
      "module.layer2.bn1.bias\n",
      "module.layer2.bn1.running_mean\n",
      "module.layer2.bn1.running_var\n",
      "module.layer2.bn1.num_batches_tracked\n",
      "module.layer2.conv2.weight\n",
      "module.layer2.conv2.bias\n",
      "module.layer2.bn2.weight\n",
      "module.layer2.bn2.bias\n",
      "module.layer2.bn2.running_mean\n",
      "module.layer2.bn2.running_var\n",
      "module.layer2.bn2.num_batches_tracked\n",
      "module.layer3.conv3.weight\n",
      "module.layer3.conv3.bias\n",
      "module.layer3.bn3.weight\n",
      "module.layer3.bn3.bias\n",
      "module.layer3.bn3.running_mean\n",
      "module.layer3.bn3.running_var\n",
      "module.layer3.bn3.num_batches_tracked\n",
      "module.layer3.conv4.weight\n",
      "module.layer3.conv4.bias\n"
     ]
    }
   ],
   "source": [
    "gen_params = generator.state_dict().keys()\n",
    "dis_params = discriminator.state_dict().keys()\n",
    "\n",
    "for i in gen_params:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-trademark",
   "metadata": {},
   "source": [
    "# 8. Set Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "focused-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr= 5*learning_rate, betas=(0.5, 0.999))\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "ones_label = Variable(torch.ones(batch_size, 1)).cuda()\n",
    "zeros_label = Variable(torch.zeros(batch_size, 1)).cuda()\n",
    "\n",
    "def image_check(gen_fake):\n",
    "    img = gen_fake.data.numpy()\n",
    "    for i in range(10):\n",
    "        plt.imshow(img[i][0], cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-christianity",
   "metadata": {},
   "source": [
    "# 9. Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "related-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------model not restored--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    generator, discriminator, = torch.load(\"./model/dcgan.pkl\")\n",
    "    print(\"\\n-----------model restored----------------\\n\")\n",
    "except:\n",
    "    print(\"\\n-----------model not restored--------------\\n\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-circular",
   "metadata": {},
   "source": [
    "# 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "regular-locator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  import sys\n",
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1965, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.19651813805103302 dis_loss: 0.6035259366035461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Discriminator. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7669, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.766917884349823 dis_loss: 0.07447990030050278\n",
      "tensor(0.5292, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.5292307734489441 dis_loss: 0.1863524168729782\n",
      "tensor(0.5764, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.1831, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.5764449834823608 dis_loss: 0.1830945760011673\n",
      "tensor(0.5387, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.1800, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.5387470126152039 dis_loss: 0.18000206351280212\n",
      "tensor(0.2675, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.2674868404865265 dis_loss: 0.31460481882095337\n",
      "tensor(0.6083, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.2719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.6083249449729919 dis_loss: 0.2718614637851715\n",
      "tensor(0.3436, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.343644380569458 dis_loss: 0.31814730167388916\n",
      "tensor(0.4172, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3744, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.4171959161758423 dis_loss: 0.37435203790664673\n",
      "tensor(0.3115, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3684, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.31151944398880005 dis_loss: 0.3684404790401459\n",
      "tensor(0.5278, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.5277788043022156 dis_loss: 0.4215647876262665\n",
      "tensor(0.2935, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.2934757471084595 dis_loss: 0.40006890892982483\n",
      "tensor(0.3898, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.3897634744644165 dis_loss: 0.40010327100753784\n",
      "tensor(0.3991, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.39912763237953186 dis_loss: 0.3834519684314728\n",
      "tensor(0.2044, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.2043946236371994 dis_loss: 0.4453558921813965\n",
      "tensor(0.4720, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4111, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.4719998240470886 dis_loss: 0.4111188054084778\n",
      "tensor(0.4128, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.41279298067092896 dis_loss: 0.39616724848747253\n",
      "tensor(0.3529, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.3529478907585144 dis_loss: 0.39758092164993286\n",
      "tensor(0.4229, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.4229350984096527 dis_loss: 0.41596707701683044\n",
      "tensor(0.2757, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.2756519019603729 dis_loss: 0.39530694484710693\n",
      "tensor(0.3623, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3967, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.3622758388519287 dis_loss: 0.39669352769851685\n",
      "tensor(0.3343, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.3976, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.33428096771240234 dis_loss: 0.3975852429866791\n",
      "tensor(0.2578, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.25775256752967834 dis_loss: 0.42470818758010864\n",
      "tensor(0.3248, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.3247986435890198 dis_loss: 0.40974563360214233\n",
      "tensor(0.4202, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4083, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.420220285654068 dis_loss: 0.40831291675567627\n",
      "tensor(0.2805, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.2805463373661041 dis_loss: 0.44200313091278076\n",
      "tensor(0.3949, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4180, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.39486464858055115 dis_loss: 0.41802865266799927\n",
      "tensor(0.2761, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4128, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.27607330679893494 dis_loss: 0.41275107860565186\n",
      "tensor(0.2690, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.26895686984062195 dis_loss: 0.4377393126487732\n",
      "tensor(0.2535, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.2535041570663452 dis_loss: 0.41910475492477417\n",
      "tensor(0.3889, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4218, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.38885730504989624 dis_loss: 0.42182689905166626\n",
      "tensor(0.4287, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.4286796748638153 dis_loss: 0.43056246638298035\n",
      "tensor(0.2329, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.23288185894489288 dis_loss: 0.4308682084083557\n",
      "tensor(0.3902, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.3902401030063629 dis_loss: 0.43018248677253723\n",
      "tensor(0.4195, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.4194785952568054 dis_loss: 0.4365795850753784\n",
      "tensor(0.2976, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.29757118225097656 dis_loss: 0.4285736680030823\n",
      "tensor(0.3385, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.33852827548980713 dis_loss: 0.4211122691631317\n",
      "tensor(0.3698, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.3698217570781708 dis_loss: 0.42389869689941406\n",
      "tensor(0.2565, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.2565173804759979 dis_loss: 0.44595181941986084\n",
      "tensor(0.2542, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4357, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.2542152404785156 dis_loss: 0.4357007145881653\n",
      "tensor(0.4260, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.4259946048259735 dis_loss: 0.4526660442352295\n",
      "tensor(0.2835, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4395, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.2835181653499603 dis_loss: 0.4395200312137604\n",
      "tensor(0.3641, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.36407938599586487 dis_loss: 0.42754921317100525\n",
      "tensor(0.3851, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4358, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.38509485125541687 dis_loss: 0.4357869029045105\n",
      "tensor(0.3947, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.39466843008995056 dis_loss: 0.43331319093704224\n",
      "tensor(0.3858, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4325, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.385762095451355 dis_loss: 0.4325070381164551\n",
      "tensor(0.3136, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.31364351511001587 dis_loss: 0.43675753474235535\n",
      "tensor(0.3136, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.31361523270606995 dis_loss: 0.41217276453971863\n",
      "tensor(0.2464, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.2464197278022766 dis_loss: 0.448386549949646\n",
      "tensor(0.2748, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.27480629086494446 dis_loss: 0.4266577959060669\n",
      "tensor(0.4217, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.42169463634490967 dis_loss: 0.4608101546764374\n",
      "tensor(0.2780, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4263, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.2779950797557831 dis_loss: 0.42632877826690674\n",
      "tensor(0.3520, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.35199522972106934 dis_loss: 0.4369429349899292\n",
      "tensor(0.3481, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.3480733335018158 dis_loss: 0.4271228313446045\n",
      "tensor(0.2673, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.2673182487487793 dis_loss: 0.43765535950660706\n",
      "tensor(0.4289, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.4289296865463257 dis_loss: 0.4366380274295807\n",
      "tensor(0.2173, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.21731603145599365 dis_loss: 0.4490053951740265\n",
      "tensor(0.3563, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4450, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.3562975823879242 dis_loss: 0.44500628113746643\n",
      "tensor(0.2620, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4366, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.26198476552963257 dis_loss: 0.43663835525512695\n",
      "tensor(0.2565, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.25654828548431396 dis_loss: 0.4420796036720276\n",
      "tensor(0.3377, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.3376660943031311 dis_loss: 0.4384445548057556\n",
      "tensor(0.2197, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4555, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.21971122920513153 dis_loss: 0.4555469751358032\n",
      "tensor(0.2821, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.28205689787864685 dis_loss: 0.4391617774963379\n",
      "tensor(0.3451, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.3451453447341919 dis_loss: 0.42922264337539673\n",
      "tensor(0.2301, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4576, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.2301279455423355 dis_loss: 0.45759671926498413\n",
      "tensor(0.2971, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4317, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.2971363663673401 dis_loss: 0.43173712491989136\n",
      "tensor(0.2892, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.28920185565948486 dis_loss: 0.44298702478408813\n",
      "tensor(0.3624, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.36239421367645264 dis_loss: 0.43760406970977783\n",
      "tensor(0.1969, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.19689889252185822 dis_loss: 0.4619499742984772\n",
      "tensor(0.2165, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.2165219485759735 dis_loss: 0.44516894221305847\n",
      "tensor(0.2456, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4381, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.24555513262748718 dis_loss: 0.438131183385849\n",
      "tensor(0.3912, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.39121493697166443 dis_loss: 0.44693756103515625\n",
      "tensor(0.2965, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.2964513301849365 dis_loss: 0.4290902614593506\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.3067869246006012 dis_loss: 0.4549020528793335\n",
      "tensor(0.3363, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.3362618684768677 dis_loss: 0.4377458095550537\n",
      "tensor(0.2649, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4399, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.26491332054138184 dis_loss: 0.4399319291114807\n",
      "tensor(0.2897, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4507, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.2897353768348694 dis_loss: 0.4507291615009308\n",
      "tensor(0.3383, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.3382995128631592 dis_loss: 0.42820799350738525\n",
      "tensor(0.3277, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.32766565680503845 dis_loss: 0.44200167059898376\n",
      "tensor(0.3090, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.30900776386260986 dis_loss: 0.45273926854133606\n",
      "tensor(0.2112, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4403, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.2111559808254242 dis_loss: 0.44027215242385864\n",
      "tensor(0.2741, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.2741386592388153 dis_loss: 0.4420127272605896\n",
      "tensor(0.3571, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.3570539057254791 dis_loss: 0.4145164489746094\n",
      "tensor(0.3699, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.369853138923645 dis_loss: 0.43441757559776306\n",
      "tensor(0.3469, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.3468695878982544 dis_loss: 0.44162601232528687\n",
      "tensor(0.2339, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.2338876724243164 dis_loss: 0.4415578544139862\n",
      "tensor(0.3544, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4299, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.35442090034484863 dis_loss: 0.42991334199905396\n",
      "tensor(0.2837, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4319, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.2837253212928772 dis_loss: 0.4319259226322174\n",
      "tensor(0.3498, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.3498094975948334 dis_loss: 0.4373105764389038\n",
      "tensor(0.2391, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.23908352851867676 dis_loss: 0.44976454973220825\n",
      "tensor(0.3644, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4159, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.3643729090690613 dis_loss: 0.41593849658966064\n",
      "tensor(0.3577, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4407, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.3576993942260742 dis_loss: 0.4406793415546417\n",
      "tensor(0.2679, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4508, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.26788637042045593 dis_loss: 0.4507802724838257\n",
      "tensor(0.3721, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.3721029758453369 dis_loss: 0.44203048944473267\n",
      "tensor(0.2659, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.2658822238445282 dis_loss: 0.4313807487487793\n",
      "tensor(0.2118, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4715, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.21179087460041046 dis_loss: 0.4715023934841156\n",
      "tensor(0.4208, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4606, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.4207562506198883 dis_loss: 0.46059155464172363\n",
      "tensor(0.3717, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.3716985583305359 dis_loss: 0.4328143298625946\n",
      "tensor(0.3491, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4430, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.34913668036460876 dis_loss: 0.4430377185344696\n",
      "tensor(0.2722, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.2722293734550476 dis_loss: 0.43727782368659973\n",
      "tensor(0.2709, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4413, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.2708519697189331 dis_loss: 0.4412599205970764\n",
      "tensor(0.2483, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4598, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.24827462434768677 dis_loss: 0.4598066806793213\n",
      "tensor(0.2512, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.25124871730804443 dis_loss: 0.4492154121398926\n",
      "tensor(0.3947, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4572, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.3947438597679138 dis_loss: 0.4571939706802368\n",
      "tensor(0.2712, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.27123332023620605 dis_loss: 0.43830031156539917\n",
      "tensor(0.3325, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4499, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.3325114846229553 dis_loss: 0.44994091987609863\n",
      "tensor(0.2959, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.295944482088089 dis_loss: 0.44061577320098877\n",
      "tensor(0.2790, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.2789788842201233 dis_loss: 0.4301893711090088\n",
      "tensor(0.3043, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.3043113350868225 dis_loss: 0.4432796835899353\n",
      "tensor(0.2817, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4362, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.2816571295261383 dis_loss: 0.4362087845802307\n",
      "tensor(0.2646, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.2646079957485199 dis_loss: 0.44653570652008057\n",
      "tensor(0.2341, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.23412130773067474 dis_loss: 0.4554382562637329\n",
      "tensor(0.2554, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.25542572140693665 dis_loss: 0.4589623808860779\n",
      "tensor(0.2983, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.2983167767524719 dis_loss: 0.447022408246994\n",
      "tensor(0.3221, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.3221486210823059 dis_loss: 0.4448166787624359\n",
      "tensor(0.3127, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4314, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.31272995471954346 dis_loss: 0.4314069151878357\n",
      "tensor(0.2742, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4660, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.27420276403427124 dis_loss: 0.4659709930419922\n",
      "tensor(0.3104, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.31043124198913574 dis_loss: 0.44687166810035706\n",
      "tensor(0.2507, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4656, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.25071367621421814 dis_loss: 0.4655652642250061\n",
      "tensor(0.3272, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4439, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.32719117403030396 dis_loss: 0.44389721751213074\n",
      "tensor(0.2991, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.2991095781326294 dis_loss: 0.44666990637779236\n",
      "tensor(0.2577, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4502, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.2577216625213623 dis_loss: 0.4501510560512543\n",
      "tensor(0.2331, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.23311549425125122 dis_loss: 0.48295265436172485\n",
      "tensor(0.2489, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.24893765151500702 dis_loss: 0.4536921977996826\n",
      "tensor(0.2503, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4643, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.25028976798057556 dis_loss: 0.46433472633361816\n",
      "tensor(0.2923, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.29225024580955505 dis_loss: 0.4473186731338501\n",
      "tensor(0.3398, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4421, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.33979326486587524 dis_loss: 0.44214528799057007\n",
      "tensor(0.3164, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4637, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.3164173662662506 dis_loss: 0.4636659622192383\n",
      "tensor(0.3468, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4628, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.3467845320701599 dis_loss: 0.46282804012298584\n",
      "tensor(0.3520, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4621, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.35200196504592896 dis_loss: 0.4620859622955322\n",
      "tensor(0.2758, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.27582821249961853 dis_loss: 0.4640156030654907\n",
      "tensor(0.3582, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4752, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.35820838809013367 dis_loss: 0.4752444922924042\n",
      "tensor(0.3254, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4624, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.32543396949768066 dis_loss: 0.46235841512680054\n",
      "tensor(0.2738, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.27380117774009705 dis_loss: 0.4574868679046631\n",
      "tensor(0.2597, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.2596703767776489 dis_loss: 0.4523225724697113\n",
      "tensor(0.3323, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4614, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.3322735130786896 dis_loss: 0.46135270595550537\n",
      "tensor(0.3420, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.34202325344085693 dis_loss: 0.4532620310783386\n",
      "tensor(0.3210, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4579, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.3210180699825287 dis_loss: 0.45786821842193604\n",
      "tensor(0.3210, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.32099178433418274 dis_loss: 0.4392448663711548\n",
      "tensor(0.3215, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.3215089440345764 dis_loss: 0.4574630856513977\n",
      "tensor(0.3664, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.36636021733283997 dis_loss: 0.46113163232803345\n",
      "tensor(0.2657, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4615, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.26566916704177856 dis_loss: 0.46153950691223145\n",
      "tensor(0.2584, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4607, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.2584441602230072 dis_loss: 0.46070414781570435\n",
      "tensor(0.3145, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4424, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.31449460983276367 dis_loss: 0.44237205386161804\n",
      "tensor(0.3086, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.308571457862854 dis_loss: 0.4543817639350891\n",
      "tensor(0.2837, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4617, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.2837173640727997 dis_loss: 0.46174538135528564\n",
      "tensor(0.3427, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.34272828698158264 dis_loss: 0.45189422369003296\n",
      "tensor(0.3149, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4540, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.3149224817752838 dis_loss: 0.4540331959724426\n",
      "tensor(0.3024, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.30240482091903687 dis_loss: 0.4612380862236023\n",
      "tensor(0.2570, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.4672, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.2570405602455139 dis_loss: 0.4671809673309326\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for j, (image, label) in enumerate(train_loader):\n",
    "        image = Variable(image).cuda()\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size, 100), mean=0, std=0.1)).cuda()\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        gen_loss = torch.sum(loss_func(dis_fake, ones_label))\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "        \n",
    "        dis_optim.zero_grad()\n",
    "        \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size, 100), mean=0, std=0.1)).cuda()\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        dis_real = discriminator.forward(image)\n",
    "        dis_loss = torch.sum(loss_func(dis_fake, zeros_label)) + torch.sum(loss_func(dis_real, ones_label))\n",
    "        dis_loss.backward()\n",
    "        dis_optim.step()\n",
    "        \n",
    "        if j % 50 == 0:\n",
    "            print(gen_loss, dis_loss)\n",
    "            torch.save([generator, discriminator], \"./model/dcgan.pkl\")\n",
    "            \n",
    "            print(\"{}th iteration gen_loss: {} dis_loss: {}\".format(i, gen_loss.data, dis_loss.data))\n",
    "            v_utils.save_image(gen_fake.data[0:25], \"./result/gen_{}_{}.png\".format(i, j), nrow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-initial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
