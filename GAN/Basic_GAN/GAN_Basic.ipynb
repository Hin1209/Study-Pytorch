{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "enclosed-mayor",
   "metadata": {},
   "source": [
    "# 1. Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "about-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils as utils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "import torchvision.utils as v_utils \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-public",
   "metadata": {},
   "source": [
    "# 2. Hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "nominated-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_gpus는 사용할 gpu의 개수 \n",
    "\n",
    "epoch = 50\n",
    "batch_size = 512\n",
    "learning_rate = 0.0002\n",
    "num_gpus = 1\n",
    "z_size = 50\n",
    "middle_size = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-thirty",
   "metadata": {},
   "source": [
    "# 3. Data Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "robust-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dset.MNIST(\"./data\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-committee",
   "metadata": {},
   "source": [
    "# 4. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "purple-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(z_size, middle_size)),\n",
    "            ('bn1', nn.BatchNorm1d(middle_size)), \n",
    "            ('act1', nn.ReLU()),\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "            ('fc2', nn.Linear(middle_size, 784)),\n",
    "            # ('bn2', nn.BatchNorm2d(784)),\n",
    "            ('tanh', nn.Tanh()),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size // num_gpus, 1, 28, 28)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-minneapolis",
   "metadata": {},
   "source": [
    "# 5. Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "intermediate-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.layer1 = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(784, middle_size)),\n",
    "            ('bn1', nn.BatchNorm1d(middle_size)),\n",
    "            ('act1', nn.LeakyReLU()),\n",
    "        ]))\n",
    "        self.layer2 = nn.Sequential(OrderedDict([\n",
    "            ('fc2', nn.Linear(middle_size, 1)),\n",
    "#             ('bn2', nn.BatchNorm2d(1)),\n",
    "            ('act2', nn.Sigmoid()),\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x.view(batch_size // num_gpus, -1)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-essence",
   "metadata": {},
   "source": [
    "# 6. Put instances on Muti-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "synthetic-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = nn.DataParallel(Generator()).cuda()\n",
    "discriminator = nn.DataParallel(Discriminator()).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-growth",
   "metadata": {},
   "source": [
    "# 7. Check layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "figured-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.fc1.weight\n",
      "module.layer1.fc1.bias\n",
      "module.layer1.bn1.weight\n",
      "module.layer1.bn1.bias\n",
      "module.layer1.bn1.running_mean\n",
      "module.layer1.bn1.running_var\n",
      "module.layer1.bn1.num_batches_tracked\n",
      "module.layer2.fc2.weight\n",
      "module.layer2.fc2.bias\n"
     ]
    }
   ],
   "source": [
    "gen_params = generator.state_dict().keys()\n",
    "dis_params = discriminator.state_dict().keys()\n",
    "\n",
    "for i in gen_params:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-giant",
   "metadata": {},
   "source": [
    "# 8. Set Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "northern-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "gen_optim = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "dis_optim = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "ones_label = Variable(torch.ones(batch_size, 1)).cuda()\n",
    "zeros_label = Variable(torch.zeros(batch_size, 1)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-circus",
   "metadata": {},
   "source": [
    "# 9. Restore Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "serial-genre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------model not restored---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    generator, discriminator = torch.load(\"./model/vanilla_gan.pkl\")\n",
    "    print(\"\\n---------model restored------------\\n\")\n",
    "except:\n",
    "    print(\"\\n------------model not restored---------\\n\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-final",
   "metadata": {},
   "source": [
    "# 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "distinct-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  \n",
      "C:\\Users\\SWCOM\\anaconda3\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2766, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5093, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.2766050100326538\n",
      "tensor(0.2721, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "0th iteration gen_loss: 0.2720579206943512\n",
      "tensor(0.2719, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.271920382976532\n",
      "tensor(0.2684, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5120, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "1th iteration gen_loss: 0.2683781683444977\n",
      "tensor(0.2686, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5122, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.2685858905315399\n",
      "tensor(0.2648, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "2th iteration gen_loss: 0.26479625701904297\n",
      "tensor(0.2635, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.2635374069213867\n",
      "tensor(0.2585, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5198, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "3th iteration gen_loss: 0.25846222043037415\n",
      "tensor(0.2588, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.25878021121025085\n",
      "tensor(0.2519, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "4th iteration gen_loss: 0.2519322335720062\n",
      "tensor(0.2503, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5269, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.2502569556236267\n",
      "tensor(0.2463, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "5th iteration gen_loss: 0.2462524175643921\n",
      "tensor(0.2450, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5375, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.24504885077476501\n",
      "tensor(0.2372, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "6th iteration gen_loss: 0.23718726634979248\n",
      "tensor(0.2332, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5402, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.233179971575737\n",
      "tensor(0.2278, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "7th iteration gen_loss: 0.22782795131206512\n",
      "tensor(0.2284, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.22844937443733215\n",
      "tensor(0.2205, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5618, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "8th iteration gen_loss: 0.2204611599445343\n",
      "tensor(0.2189, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5608, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.21891751885414124\n",
      "tensor(0.2104, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5720, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "9th iteration gen_loss: 0.21038223803043365\n",
      "tensor(0.2115, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.21154509484767914\n",
      "tensor(0.2042, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5808, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "10th iteration gen_loss: 0.20417673885822296\n",
      "tensor(0.2033, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5825, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.20332196354866028\n",
      "tensor(0.1963, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5876, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "11th iteration gen_loss: 0.19629690051078796\n",
      "tensor(0.1935, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.5943, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.19345375895500183\n",
      "tensor(0.1879, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6009, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "12th iteration gen_loss: 0.18792887032032013\n",
      "tensor(0.1849, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.18491265177726746\n",
      "tensor(0.1790, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "13th iteration gen_loss: 0.17900587618350983\n",
      "tensor(0.1790, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6155, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.1790127009153366\n",
      "tensor(0.1684, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6273, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "14th iteration gen_loss: 0.16836093366146088\n",
      "tensor(0.1693, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.169333353638649\n",
      "tensor(0.1625, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6396, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "15th iteration gen_loss: 0.16253897547721863\n",
      "tensor(0.1579, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.157907634973526\n",
      "tensor(0.1501, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "16th iteration gen_loss: 0.15012392401695251\n",
      "tensor(0.1497, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.14965853095054626\n",
      "tensor(0.1421, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "17th iteration gen_loss: 0.14210742712020874\n",
      "tensor(0.1403, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6689, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.14029864966869354\n",
      "tensor(0.1350, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "18th iteration gen_loss: 0.13496509194374084\n",
      "tensor(0.1335, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6891, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.13345573842525482\n",
      "tensor(0.1269, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6939, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "19th iteration gen_loss: 0.12688952684402466\n",
      "tensor(0.1260, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6987, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.12598638236522675\n",
      "tensor(0.1212, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "20th iteration gen_loss: 0.12119275331497192\n",
      "tensor(0.1185, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.11853085458278656\n",
      "tensor(0.1166, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7191, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "21th iteration gen_loss: 0.11659654229879379\n",
      "tensor(0.1141, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.11413843929767609\n",
      "tensor(0.1112, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7249, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "22th iteration gen_loss: 0.11120299994945526\n",
      "tensor(0.1100, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.10997186601161957\n",
      "tensor(0.1024, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "23th iteration gen_loss: 0.10241767019033432\n",
      "tensor(0.1037, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7442, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.10371995717287064\n",
      "tensor(0.1012, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "24th iteration gen_loss: 0.10115564614534378\n",
      "tensor(0.0999, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7454, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.09991829842329025\n",
      "tensor(0.0958, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "25th iteration gen_loss: 0.09580376744270325\n",
      "tensor(0.0989, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.09887632727622986\n",
      "tensor(0.0929, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "26th iteration gen_loss: 0.0929083526134491\n",
      "tensor(0.0941, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7650, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.09412116557359695\n",
      "tensor(0.0918, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "27th iteration gen_loss: 0.0917787030339241\n",
      "tensor(0.0911, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7644, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.09111020714044571\n",
      "tensor(0.0864, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7685, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "28th iteration gen_loss: 0.08642368018627167\n",
      "tensor(0.0826, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7779, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.08255161345005035\n",
      "tensor(0.0837, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7863, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "29th iteration gen_loss: 0.08373421430587769\n",
      "tensor(0.0850, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.08500458300113678\n",
      "tensor(0.0806, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7935, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "30th iteration gen_loss: 0.0806489959359169\n",
      "tensor(0.0809, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.08089207112789154\n",
      "tensor(0.0782, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7919, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "31th iteration gen_loss: 0.07815416157245636\n",
      "tensor(0.0775, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.07751574367284775\n",
      "tensor(0.0783, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8014, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "32th iteration gen_loss: 0.07832357287406921\n",
      "tensor(0.0738, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8040, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.0737815871834755\n",
      "tensor(0.0762, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.7974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "33th iteration gen_loss: 0.0762157067656517\n",
      "tensor(0.0772, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.07723656296730042\n",
      "tensor(0.0775, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "34th iteration gen_loss: 0.07752154767513275\n",
      "tensor(0.0755, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8171, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.07550408691167831\n",
      "tensor(0.0716, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8215, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "35th iteration gen_loss: 0.07161940634250641\n",
      "tensor(0.0717, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8136, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.07170382142066956\n",
      "tensor(0.0678, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "36th iteration gen_loss: 0.06775932013988495\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8145, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.07118106633424759\n",
      "tensor(0.0675, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8183, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "37th iteration gen_loss: 0.0674954354763031\n",
      "tensor(0.0728, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.07275089621543884\n",
      "tensor(0.0696, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8293, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "38th iteration gen_loss: 0.06960442662239075\n",
      "tensor(0.0718, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.07178986072540283\n",
      "tensor(0.0671, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "39th iteration gen_loss: 0.06709211319684982\n",
      "tensor(0.0747, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8173, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.07472995668649673\n",
      "tensor(0.0629, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "40th iteration gen_loss: 0.06292800605297089\n",
      "tensor(0.0695, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.0695071816444397\n",
      "tensor(0.0616, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "41th iteration gen_loss: 0.061619024723768234\n",
      "tensor(0.0684, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8305, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.06840848177671432\n",
      "tensor(0.0635, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8558, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "42th iteration gen_loss: 0.06346847116947174\n",
      "tensor(0.0604, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.06038388982415199\n",
      "tensor(0.0608, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "43th iteration gen_loss: 0.06083889305591583\n",
      "tensor(0.0598, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.05984892323613167\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8270, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "44th iteration gen_loss: 0.06577084958553314\n",
      "tensor(0.0622, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.06223594769835472\n",
      "tensor(0.0575, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8466, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "45th iteration gen_loss: 0.057481780648231506\n",
      "tensor(0.0620, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.06203559786081314\n",
      "tensor(0.0558, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "46th iteration gen_loss: 0.055813126266002655\n",
      "tensor(0.0589, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8582, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.0588635578751564\n",
      "tensor(0.0601, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "47th iteration gen_loss: 0.060120631009340286\n",
      "tensor(0.0621, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8398, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.06214268133044243\n",
      "tensor(0.0588, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8521, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "48th iteration gen_loss: 0.05877366662025452\n",
      "tensor(0.0617, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8584, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.06170684099197388\n",
      "tensor(0.0565, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.8704, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "49th iteration gen_loss: 0.05649779364466667\n"
     ]
    }
   ],
   "source": [
    "for i in range(epoch):\n",
    "    for j, (image, label) in enumerate(train_loader):\n",
    "        image = Variable(image).cuda()\n",
    "        \n",
    "        dis_optim.zero_grad()\n",
    "        \n",
    "        # 노이즈 z를 생성하고 fake 데이터를 generator를 통해 만듬 \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size, z_size), mean=0, std=0.1)).cuda()\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        dis_real = discriminator.forward(image)\n",
    "        # 실제 데이터와 가짜 데이터에 대한 손실값을 더함 \n",
    "        dis_loss = torch.sum(loss_func(dis_fake, zeros_label)) + torch.sum(loss_func(dis_real, ones_label))\n",
    "        \n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        \n",
    "        z = Variable(init.normal(torch.Tensor(batch_size, z_size), mean=0, std=0.1)).cuda()\n",
    "        gen_fake = generator.forward(z)\n",
    "        dis_fake = discriminator.forward(gen_fake)\n",
    "        \n",
    "        gen_loss = torch.sum(loss_func(dis_fake, ones_label))\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "        \n",
    "        \n",
    "        if j % 100 == 0:\n",
    "            print(gen_loss, dis_loss)\n",
    "            torch.save([generator, discriminator], './model/vanilla_gan.pkl')\n",
    "            \n",
    "            print(\"{}th iteration gen_loss: {}\".format(i, gen_loss.data, dis_loss.data))\n",
    "            v_utils.save_image(gen_fake.data[0:25], \"./result/gen_{}_{}.png\".format(i, j), nrow=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
